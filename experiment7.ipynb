{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "509ae77d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseError",
     "evalue": "unclosed token: line 36, column 4 (<string>)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[0;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3505\u001b[0m in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[0;32mIn[11], line 2\u001b[0m\n    nltk.download('punkt', force=True)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32m~\\anaconda3\\Lib\\site-packages\\nltk\\downloader.py:777\u001b[0m in \u001b[0;35mdownload\u001b[0m\n    for msg in self.incr_download(info_or_id, download_dir, force):\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32m~\\anaconda3\\Lib\\site-packages\\nltk\\downloader.py:629\u001b[0m in \u001b[0;35mincr_download\u001b[0m\n    info = self._info_or_id(info_or_id)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32m~\\anaconda3\\Lib\\site-packages\\nltk\\downloader.py:603\u001b[0m in \u001b[0;35m_info_or_id\u001b[0m\n    return self.info(info_or_id)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32m~\\anaconda3\\Lib\\site-packages\\nltk\\downloader.py:1009\u001b[0m in \u001b[0;35minfo\u001b[0m\n    self._update_index()\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32m~\\anaconda3\\Lib\\site-packages\\nltk\\downloader.py:952\u001b[0m in \u001b[0;35m_update_index\u001b[0m\n    ElementTree.parse(urlopen(self._url)).getroot()\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32m~\\anaconda3\\Lib\\xml\\etree\\ElementTree.py:1218\u001b[0m in \u001b[0;35mparse\u001b[0m\n    tree.parse(source, parser)\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m~\\anaconda3\\Lib\\xml\\etree\\ElementTree.py:580\u001b[1;36m in \u001b[1;35mparse\u001b[1;36m\n\u001b[1;33m    self._root = parser._parse_whole(source)\u001b[1;36m\n",
      "\u001b[1;36m  File \u001b[1;32m<string>\u001b[1;36m\u001b[0m\n\u001b[1;31mParseError\u001b[0m\u001b[1;31m:\u001b[0m unclosed token: line 36, column 4\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt', force=True)\n",
    "\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "# from nltk import pos_tag\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# def preprocess_document(document):\n",
    "#     # Tokenization\n",
    "#     tokens = word_tokenize(document.lower())\n",
    "\n",
    "#     # POS Tagging\n",
    "#     pos_tags = pos_tag(tokens)\n",
    "\n",
    "#     # Stop words removal\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     filtered_tokens = [token for token, pos in pos_tags if token not in stop_words]\n",
    "\n",
    "#     # Stemming\n",
    "#     stemmer = PorterStemmer()\n",
    "#     stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "\n",
    "#     # Lemmatization\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "\n",
    "#     return stemmed_tokens, lemmatized_tokens\n",
    "\n",
    "# def calculate_tf_idf(documents):\n",
    "#     tfidf_vectorizer = TfidfVectorizer()\n",
    "#     tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "#     feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "#     return tfidf_matrix, feature_names\n",
    "\n",
    "# # Example document\n",
    "# document = \"The quick brown fox jumps over the lazy dog. The dog sleeps in the sun.\"\n",
    "\n",
    "# # Preprocess document\n",
    "# stemmed_tokens, lemmatized_tokens = preprocess_document(document)\n",
    "\n",
    "# # Print preprocessed tokens\n",
    "# print(\"Stemmed tokens:\", stemmed_tokens)\n",
    "# print(\"Lemmatized tokens:\", lemmatized_tokens)\n",
    "\n",
    "# # Calculate TF-IDF representation\n",
    "# tfidf_matrix, feature_names = calculate_tf_idf([document])\n",
    "\n",
    "# # Print TF-IDF representation\n",
    "# print(\"\\nTF-IDF Matrix:\")\n",
    "# print(tfidf_matrix.toarray())\n",
    "# print(\"\\nFeature Names:\", feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1351998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\admin\\anaconda3\\lib\\site-packages (3.7.4)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (1.24.3)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "563f4487",
   "metadata": {},
   "outputs": [],
   "source": [
    "#python -m spacy download en_core_web_sm   \n",
    "\n",
    "import spacy\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bdbb017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pranjal is girl.\n",
      "Tom is a boy.\n"
     ]
    }
   ],
   "source": [
    "with open('1.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read().strip()\n",
    "doc = nlp(text)\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c99f42da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Pranjal', 'is', 'girl', '.', '\\n', 'Tom', 'is', 'a', 'boy', '.']\n"
     ]
    }
   ],
   "source": [
    "#tokenization\n",
    "tokens = [token.text for token in doc]\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06a1f780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tags: [('Pranjal', 'PROPN'), ('is', 'AUX'), ('girl', 'NOUN'), ('.', 'PUNCT'), ('\\n', 'SPACE'), ('Tom', 'PROPN'), ('is', 'AUX'), ('a', 'DET'), ('boy', 'NOUN'), ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "#pos tagging\n",
    "pos_tags = [(token.text, token.pos_) for token in doc]\n",
    "print(\"POS Tags:\", pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac3d6f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Stop Words Removal: ['Pranjal', 'girl', '.', '\\n', 'Tom', 'boy', '.']\n"
     ]
    }
   ],
   "source": [
    "#stop words\n",
    "stop_words_removed = [token.text for token in doc if not token.is_stop]\n",
    "print(\"After Stop Words Removal:\", stop_words_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46262bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming and Lemmatization: [('Pranjal', 'Pranjal', 'pranjal', 'pranjal'), ('is', 'be', 'be', 'is'), ('girl', 'girl', 'girl', 'girl'), ('.', '.', '.', '.'), ('\\n', '\\n', '\\n', '\\n'), ('Tom', 'Tom', 'tom', 'tom'), ('is', 'be', 'be', 'is'), ('a', 'a', 'a', 'a'), ('boy', 'boy', 'boy', 'boy'), ('.', '.', '.', '.')]\n"
     ]
    }
   ],
   "source": [
    "#steemming and lemmitization\n",
    "stem_and_lemma = [(token.text, token.lemma_, token.lemma_.lower(), token.norm_) for token in doc]\n",
    "print(\"Stemming and Lemmatization:\", stem_and_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8df8148e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix:\n",
      "[[0.6316672  0.         0.6316672  0.44943642 0.        ]\n",
      " [0.         0.6316672  0.         0.44943642 0.6316672 ]]\n",
      "\n",
      "Feature Names:\n",
      "['boy' 'girl' 'he' 'is' 'she']\n"
     ]
    }
   ],
   "source": [
    "#Q2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"He is a boy.\",\n",
    "    \"She is a girl.\"\n",
    "]\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get feature names (words)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Display TF-IDF matrix\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print(tfidf_matrix.toarray())\n",
    "\n",
    "# Display feature names\n",
    "print(\"\\nFeature Names:\")\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c60228d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (3.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bdcf7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-docx in c:\\users\\admin\\anaconda3\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: lxml<=4.9.2,>=3.1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from python-docx) (4.9.2)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from python-docx) (4.11.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d9b2c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "<bound method PageObject.extract_text of {'/Annots': IndirectObject(11, 0, 2444029718544), '/ArtBox': [0, 0, 595.276, 841.89], '/BleedBox': [0, 0, 595.276, 841.89], '/Contents': [IndirectObject(21, 0, 2444029718544), IndirectObject(22, 0, 2444029718544), IndirectObject(23, 0, 2444029718544), IndirectObject(24, 0, 2444029718544), IndirectObject(25, 0, 2444029718544), IndirectObject(26, 0, 2444029718544), IndirectObject(27, 0, 2444029718544), IndirectObject(28, 0, 2444029718544)], '/CropBox': [0, 0, 595.276, 841.89], '/Group': IndirectObject(64, 0, 2444029718544), '/MediaBox': [0, 0, 595.276, 841.89], '/Parent': IndirectObject(5, 0, 2444029718544), '/PieceInfo': {'/InDesign': {'/DocumentID': 'xmp.did:b47e2f57-0029-45c5-8e1d-97f7c1535615', '/LastModified': 'D:20201014150810Z', '/NumberofPages': 1, '/OriginalDocumentID': 'xmp.did:d36a16ec-5a63-4b4a-81f5-1cc9f49b1966', '/PageUIDList': {'/0': 211}, '/PageWidthList': {'/0': 595.276}}}, '/Resources': {'/ColorSpace': {'/CS0': IndirectObject(20, 0, 2444029718544)}, '/ExtGState': {'/GS0': IndirectObject(30, 0, 2444029718544), '/GS1': IndirectObject(31, 0, 2444029718544), '/GS2': IndirectObject(32, 0, 2444029718544), '/GS3': IndirectObject(33, 0, 2444029718544), '/GS4': IndirectObject(34, 0, 2444029718544), '/GS5': IndirectObject(35, 0, 2444029718544)}, '/Font': {'/C2_0': IndirectObject(17, 0, 2444029718544), '/TT0': IndirectObject(18, 0, 2444029718544), '/TT1': IndirectObject(19, 0, 2444029718544)}, '/ProcSet': ['/PDF', '/Text'], '/XObject': {'/Fm0': IndirectObject(50, 0, 2444029718544), '/Fm1': IndirectObject(52, 0, 2444029718544), '/Fm2': IndirectObject(54, 0, 2444029718544), '/Fm3': IndirectObject(56, 0, 2444029718544), '/Fm4': IndirectObject(58, 0, 2444029718544)}}, '/Rotate': 0, '/Tabs': '/W', '/Thumb': IndirectObject(4, 0, 2444029718544), '/TrimBox': [0, 0, 595.276, 841.89], '/Type': '/Page'}>\n"
     ]
    }
   ],
   "source": [
    "# importing required modules \n",
    "import PyPDF2 \n",
    "    \n",
    "# creating a pdf file object \n",
    "pdfFileObj = open(r\"../Downloads/sample1.pdf\", 'rb') \n",
    "    \n",
    "# creating a pdf reader object \n",
    "pdfReader = PyPDF2.PdfReader(pdfFileObj) \n",
    "    \n",
    "# printing number of pages in pdf file \n",
    "print(len((pdfReader.pages)))\n",
    "      \n",
    "\n",
    "    \n",
    "# creating a page object \n",
    "pageObj = pdfReader.pages[0]\n",
    "    \n",
    "# extracting text from page \n",
    "print(pageObj.extract_text) \n",
    "    \n",
    "# closing the pdf file object \n",
    "pdfFileObj.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c95d4904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import docx NOT python-docx\n",
    "import docx\n",
    "  \n",
    "# create an instance of a word document\n",
    "doc = docx.Document()\n",
    "  \n",
    "# add a heading of level 0 (largest heading)\n",
    "doc.add_heading('Heading for the document', 0)\n",
    "  \n",
    "# add a paragraph and store \n",
    "# the object in a variable\n",
    "doc_para = doc.add_paragraph('Your paragraph goes here, ')\n",
    "  \n",
    "# add a run i.e, style like \n",
    "# bold, italic, underline, etc.\n",
    "doc_para.add_run('hey there, bold here').bold = True\n",
    "doc_para.add_run(', and ')\n",
    "doc_para.add_run('these words are italic').italic = True\n",
    "  \n",
    "# add a page break to start a new page\n",
    "doc.add_page_break()\n",
    "  \n",
    "# add a heading of level 2\n",
    "doc.add_heading('Heading level 2', 2)\n",
    "  \n",
    "# pictures can also be added to our word document\n",
    "# width is optional\n",
    "doc.add_picture(r\"../Downloads/index.jpg\")\n",
    "  \n",
    "# now save the document to a location\n",
    "doc.save('new_doc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ece505",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
